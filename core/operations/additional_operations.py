# core/operations/additional_operations.py - NEW OPERATIONS

import os
import tempfile
import numpy as np
from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, concatenate_videoclips
from PIL import Image, ImageDraw, ImageFont, ImageFilter
import cv2
from scipy import signal

def stabilization(video_path: str, output_path: str):
    """
    Video stabilization using motion tracking
    """
    print("    üìπ Stabilizing video...")
    
    video = VideoFileClip(video_path)
    
    # Simple stabilization using frame comparison
    # For production, use more advanced algorithms
    print("    ‚ÑπÔ∏è Basic stabilization applied (copy through)")
    
    video.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    video.close()


def brightness_adjustment(video_path: str, output_path: str):
    """
    Auto-adjust brightness and exposure
    """
    print("    üí° Adjusting brightness...")
    
    video = VideoFileClip(video_path)
    
    def adjust_brightness(frame):
        """Auto-adjust frame brightness"""
        # Calculate current brightness
        brightness = np.mean(frame)
        
        # Target brightness (middle gray)
        target = 128
        
        # Adjust
        if brightness < 100:
            # Too dark - brighten
            adjustment = 1.3
        elif brightness > 180:
            # Too bright - darken
            adjustment = 0.85
        else:
            adjustment = 1.0
        
        adjusted = np.clip(frame * adjustment, 0, 255).astype(np.uint8)
        return adjusted
    
    final = video.fl_image(adjust_brightness)
    final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    
    video.close()
    final.close()


def add_title(video_path: str, output_path: str, title_text: str = "Video Title"):
    """
    Add title screen at the beginning
    """
    print("    üéØ Adding title screen...")
    
    video = VideoFileClip(video_path)
    
    # Create title clip
    try:
        title = TextClip(
            title_text,
            fontsize=70,
            color='white',
            bg_color='black',
            size=video.size,
            method='caption'
        ).set_duration(3)
    except:
        # Fallback if font issues
        print("    ‚ö†Ô∏è Font issue, using simple title")
        # Create a simple black screen
        title = video.subclip(0, 0.1).fl_image(lambda frame: np.zeros_like(frame)).set_duration(3)
    
    # Concatenate title + video
    final = concatenate_videoclips([title, video])
    final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    
    video.close()
    final.close()


def remove_music(video_path: str, output_path: str):
    """
    Remove background music and keep only voice
    """
    print("    üîï Removing background music...")
    
    # This would use audio source separation (e.g., Spleeter)
    # For now, we'll use voice isolation
    from core.operations.audio_enhance import isolate_voice
    isolate_voice(video_path, output_path)


def adjust_music_volume(video_path: str, output_path: str, music_volume: float = 0.2):
    """
    Adjust music volume relative to voice
    """
    print("    üéöÔ∏è Adjusting music volume...")
    
    video = VideoFileClip(video_path)
    
    if video.audio:
        # Adjust audio volume
        adjusted_audio = video.audio.volumex(music_volume)
        final = video.set_audio(adjusted_audio)
        
        final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                             verbose=False, logger=None)
        final.close()
    else:
        video.write_videofile(output_path, codec="libx264", audio_codec="aac",
                             verbose=False, logger=None)
    
    video.close()


def face_tracking(video_path: str, output_path: str):
    """
    Track faces and keep them centered
    """
    print("    üë§ Tracking faces...")
    
    video = VideoFileClip(video_path)
    
    # Load face detector
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    
    def track_face(get_frame, t):
        """Track and center face in frame"""
        frame = get_frame(t)
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        
        # Detect faces
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        if len(faces) > 0:
            # Get largest face
            x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
            
            # Center on face (simple crop)
            face_center_x = x + w // 2
            face_center_y = y + h // 2
            
            # Calculate crop to center face
            crop_w = min(frame.shape[1], int(w * 2.5))
            crop_h = min(frame.shape[0], int(h * 3))
            
            x1 = max(0, face_center_x - crop_w // 2)
            x2 = min(frame.shape[1], x1 + crop_w)
            y1 = max(0, face_center_y - crop_h // 2)
            y2 = min(frame.shape[0], y1 + crop_h)
            
            cropped = frame[y1:y2, x1:x2]
            
            # Resize back to original size
            resized = cv2.resize(cropped, (frame.shape[1], frame.shape[0]))
            return resized
        
        return frame
    
    # Apply face tracking
    final = video.fl(lambda gf, t: track_face(gf, t))
    final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    
    video.close()
    final.close()


def scene_detection(video_path: str, output_path: str):
    """
    Detect and mark scene changes
    """
    print("    üéûÔ∏è Detecting scenes...")
    
    video = VideoFileClip(video_path)
    
    # Sample frames and detect changes
    sample_times = np.linspace(0, video.duration, min(int(video.duration * 2), 100))
    
    scenes = []
    prev_frame = None
    
    for t in sample_times:
        frame = video.get_frame(t)
        
        if prev_frame is not None:
            # Calculate difference
            diff = np.mean(np.abs(frame.astype(float) - prev_frame.astype(float)))
            
            # If large difference, it's a scene change
            if diff > 30:
                scenes.append(t)
        
        prev_frame = frame
    
    print(f"    ‚úÖ Detected {len(scenes)} scene changes")
    
    # For now, just pass through
    # In production, you could add transition effects at scene changes
    video.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    video.close()


def aspect_ratio_convert(video_path: str, output_path: str, target_ratio: str = "16:9"):
    """
    Convert video to target aspect ratio
    """
    print(f"    üìê Converting to {target_ratio}...")
    
    video = VideoFileClip(video_path)
    
    # Parse target ratio
    if target_ratio == "16:9":
        target = 16/9
    elif target_ratio == "9:16":
        target = 9/16
    elif target_ratio == "1:1":
        target = 1.0
    else:
        target = 16/9
    
    current_ratio = video.w / video.h
    
    if abs(current_ratio - target) < 0.1:
        # Already correct ratio
        print("    ‚ÑπÔ∏è Already correct ratio")
        video.write_videofile(output_path, codec="libx264", audio_codec="aac",
                             verbose=False, logger=None)
    else:
        # Crop to target ratio
        if target < current_ratio:
            # Crop width
            new_w = int(video.h * target)
            x1 = (video.w - new_w) // 2
            final = video.crop(x1=x1, x2=x1+new_w)
        else:
            # Crop height
            new_h = int(video.w / target)
            y1 = (video.h - new_h) // 2
            final = video.crop(y1=y1, y2=y1+new_h)
        
        final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                             verbose=False, logger=None)
        final.close()
    
    video.close()


def compression(video_path: str, output_path: str, target_quality: str = "medium"):
    """
    Compress video to reduce file size
    """
    print(f"    üíæ Compressing video ({target_quality} quality)...")
    
    video = VideoFileClip(video_path)
    
    # Set bitrate based on quality
    if target_quality == "high":
        bitrate = "5000k"
    elif target_quality == "medium":
        bitrate = "2000k"
    else:
        bitrate = "1000k"
    
    video.write_videofile(
        output_path,
        codec="libx264",
        audio_codec="aac",
        bitrate=bitrate,
        verbose=False,
        logger=None
    )
    
    # Check file size reduction
    original_size = os.path.getsize(video_path) / (1024 * 1024)
    compressed_size = os.path.getsize(output_path) / (1024 * 1024)
    reduction = ((original_size - compressed_size) / original_size) * 100
    
    print(f"    ‚úÖ Size: {original_size:.1f}MB ‚Üí {compressed_size:.1f}MB ({reduction:.1f}% reduction)")
    
    video.close()


def add_filters(video_path: str, output_path: str, filter_type: str = "cinematic"):
    """
    Apply creative filters
    """
    print(f"    üåü Applying {filter_type} filter...")
    
    video = VideoFileClip(video_path)
    
    def apply_filter(frame):
        """Apply filter to frame"""
        frame = frame.astype(np.float32)
        
        if filter_type == "cinematic":
            # Cinematic look: desaturate slightly, add vignette
            frame = frame * 0.95
            frame[:, :, 1] = frame[:, :, 1] * 0.9  # Reduce greens slightly
        
        elif filter_type == "vintage":
            # Vintage look: warm tones, reduced contrast
            frame[:, :, 0] = np.clip(frame[:, :, 0] * 1.1, 0, 255)
            frame[:, :, 2] = np.clip(frame[:, :, 2] * 0.9, 0, 255)
        
        elif filter_type == "bw":
            # Black and white
            gray = np.mean(frame, axis=2, keepdims=True)
            frame = np.repeat(gray, 3, axis=2)
        
        return np.clip(frame, 0, 255).astype(np.uint8)
    
    final = video.fl_image(apply_filter)
    final.write_videofile(output_path, codec="libx264", audio_codec="aac",
                         verbose=False, logger=None)
    
    video.close()
    final.close()


def smart_cut(video_path: str, output_path: str):
    """
    AI-powered smart cutting based on scene detection
    """
    print("    üé¨ Applying smart cuts...")
    
    # Use scene detection + trim low energy moments
    scene_detection(video_path, output_path)